% !TeX spellcheck = en_GB
\documentclass[12pt,fleqn]{article}

\usepackage[english]{babel}
\usepackage{SpeedyGonzales}
\usepackage{MediocreMike}
%\usepackage{Blastoise}

\title{Image Segmentation}
\author{}
\date{\today}

\pagestyle{fancy}
\fancyhf{}
\lhead{}
\chead{}
\rhead{}
\rfoot{Side \thepage{} af \pageref{LastPage}}

\graphicspath{{Billeder/}}
\linespread{1.15}


%\numberwithin{equation}{section}
%\numberwithin{footnote}{section}
%\numberwithin{figure}{section}
%\numberwithin{table}{section}

\begin{document}

%\maketitle
%\thispagestyle{fancy}

\begin{titlepage}
	\begin{center}
		\textsc{\LARGE Image Segmentation}\\
		[1.0cm]
		{
		\large
		\begin{tabular}{lr}
			Anders Henriksen&s183904\\
			Asger Schultz&s183912\\
			Oskar Wiese&s183917\\
			Mads Andersen&\\
			SÃ¸ren Winkel Holm&s183911
		\end{tabular}
		}\\
		[0.5cm]
		\textsc{\large \today}
	\end{center}
\end{titlepage}
\tableofcontents \newpage


\section{Introduction}
One of the key motivation in choosing to work with deep neural networks, more specifically
SegNet, is the adaptability of the algorithm as well as the wide application. SegNet can be
used to segment images to speed up manual work, such as cell assessment by a doctor for
cancer cells or counting invasive bird species. Using SegNet in autonomous vehicles is also
highly desirable in order to classify objects such as people, houses, trees etc. In this project SegNet is used to classify crops in arieal drone images. 

\section{Methods}
\subsection{Architecture}
The architecture of the SegNet has adopted the hourglass shaped encoder-decoder 
framework. The layers of the decoder and the encoder are symmetrical to one 
another, as seen in \ref{fig:Structure}. The skip conenctions used are the 
maxpool indices of the encoder layers. During upsampling, the maxpool indicies 
from the encoder-layers are used for upsampling in the coresponding 
decoder-layer. The encoder part of the network creates a rich feature map representing the image content. 

\subsection{Skip Connections and Max-pool}
The more layers of max-pooling there are the more translation invariance for robust classification can be achieved. The boundary detail is very important when dealing with image segmentation. Hence, capturing boundary information in the feature maps of the encoder before upsampling is important. This can 
simply be done by storing the whole feature map, but due to memory 
constrains only the maxpooling indices are saved, which is a good 
approximation of the feature maps. 

\\
\\
\subsection{Loss function: Quality over quantity}
Multi-class cross entropy because:
\begin{itemize}
	\item Softmax Network: Minus log likelihood
	\item Can be seen as a classic multiclass classifier -- just on a pixel-by-pixel basis.
\end{itemize}
Weighted cross entropy because:
\begin{itemize}
	\item Unbalanced class distribution: Network has to learn to focus on important pixels: Don't classify everything as dirt.
	\item Initial tests made the network behave as the baseline: Simple features in early layers got were not penalized enough and learning was not stable.
	\item Resampling expensive
\end{itemize}
\subsection{Metrics}
Had to use different metrics because
\begin{itemize}
	\item Not agreement in Image Segmentation papers.
	\item Want to get accuracy on a global scale and on a class scale.
	\item Different metrics important in different fields.
\end{itemize}
The metrics \footnote{https://hal.inria.fr/hal-01581525/document}\footnote{
http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf} 
\begin{itemize}
	\item Global accuracy: Trivial and  the most important because of class imbalance but is good for smoothness
	\item Mean class-wise accuracy: Takes class imbalance into account. Is what is being optimized for in the model.
	\item Mean Intersect over Union: "Jaccard Index".  Found to be better correlated with human classification though still only \(\approx 0.5\). Favours region smoothness highly and not boundary accuracy.
	\item Harmonic mean of precision and recall. To compare to others with same project. Penalizes false positives and gives less credit to true negatives thus being better for unbalanced classes.
\end{itemize}

\subsection{Regularization and Hyperparameters}
Regularization
\begin{itemize}
	\item NN's are prone to overfitting, because they are so flexible
	\item Prevent overfitting $ \to $ better results on test data
	\item Three methods
	\item Dropout: Randomly remove nodes to increase variability. $ p=25\pro $
	\item Data augmentation: Increase size of dataset
	\begin{itemize}
		\item Crop each $ 512\times 512 $ to random $ 250\times 250 $
		\item 50\pro chance of flip T/D and 50\pro chance of flip L/R
	\end{itemize}
	\item Batch normalization
	\begin{itemize}
		\item Faster convergence
		\item Prevents ReLU from not learning
		\item Introduces noise
		\item Reduces vanishing/exploding gradient problem, as values stay close to 0
	\end{itemize}
\end{itemize}
Hyperparameters
\begin{itemize}
	\item Adaptive learning rate from ADAM optimizer, initialized at $ 2\ctp{-4} $
	\item Total: 26 conv + batchnorm + ReLU, 5 pool/upsample, 1 softmax
	\item 14.7 M parameters in encoder -- significantly lower than 134 M in VGG16 because of no fully-connected layers
	\item Kernel size: $ 3\times 3 $, stride 1, maxpool: $ 2\times 2 $, stride 2
	\item Corresponding padding of 1 to prevent reduction of image size
\end{itemize}
\texttt{https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/}\\
\texttt{https://medium.com/deeper-learning/glossary-of-deep-learning-batch-normalisation-8266dcd2fa82}

\section{Results}

\section{Discussion}

\end{document}



















